{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de los usuarios I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Relationship, NodeMatcher, Node\n",
    "from multiprocessing import Process\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import collections\n",
    "\n",
    "auth_tweepy = {\"access_secret\" : \"-\",\n",
    "               \"consumer_secret\" : \"-\",\n",
    "               \"consumer_key\" : \"-\",\n",
    "               \"access_token\" : \"-\"}\n",
    "\n",
    "auth_neo4j = {\"host\" : \"http://localhost:7474\",\n",
    "              \"password\" : \"-\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recogida de los tweets de los usuarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos del fichero tomado en diciembre de 2020 filtrando los usuarios del dataset 'labeled' de *HaterNet*. En total son **3600 usuarios** los que a esa fecha estaban disponibles, por lo que es posible que hoy haya algunos usuarios suspendidos más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Dataset/full_data/tweet_hr_label_6K.csv')\n",
    "authors = df.author_id.tolist()\n",
    "len(set(authors)) # quitamos las posibles repeticiones de los usuarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recogida de los tweets de los usuarios 2.0\n",
    "La red social previa no era muy fuerte por lo que se procede a recoger más tweets procedentes de los recogidos individualmente como HS, y buscaremos en sus contactos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "743"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datos_final_743/usuarios_caracteristicas_finales.csv')\n",
    "authors = df.user_id.tolist()\n",
    "\n",
    "len(set(authors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos una recogida de 200 tweets reciente por cada uno de estos usuarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neo4j Schema\n",
    "repl = partial(re.sub, '( |\\n|\\t)+', ' ')\n",
    "\n",
    "def neo4j_tweet(tweet):\n",
    "    return Node(\"Tweet\", content=tweet)\n",
    "\n",
    "def neo4j_multimedia(multimedia):\n",
    "    return Node(\"Multimedia\", url=multimedia[0], netloc=multimedia[1], path=multimedia[2])\n",
    "\n",
    "def neo4j_vuser(user):\n",
    "    return Node(\"User\", id=user, virtual='T')\n",
    "\n",
    "def neo4j_user(user, n):\n",
    "    return Node(\"User\", \n",
    "                id=user.id, \n",
    "                uname=user.name,\n",
    "                virtual='F',\n",
    "                screen_name=user.screen_name,\n",
    "                description=repl(user.description),\n",
    "                location=user.location,\n",
    "                profile_image_url=user.profile_image_url,\n",
    "                default_profile=user.default_profile,\n",
    "                default_profile_image=user.default_profile_image,\n",
    "                geo_enabled=user.geo_enabled,\n",
    "                created_at=user.created_at.timestamp(),\n",
    "                verified=user.verified,\n",
    "                statuses_count=user.statuses_count, \n",
    "                followers_count=user.followers_count,\n",
    "                followees_count=user.friends_count,\n",
    "                favorites_count=user.favourites_count,\n",
    "                listed_count=user.listed_count,\n",
    "                number=n)\n",
    "\n",
    "def neo4j_save_info(bd_user, user, n):\n",
    "    bd_user['n'] = n\n",
    "    bd_user['uname'] = user.name\n",
    "    bd_user['virtual'] = 'F'\n",
    "    bd_user['screen_name'] = user.screen_name\n",
    "    bd_user['description'] = user.description\n",
    "    bd_user['location'] = user.location\n",
    "    bd_user['profile_image_url'] = user.profile_image_url\n",
    "    bd_user['default_profile'] = user.default_profile\n",
    "    bd_user['default_profile_image'] = user.default_profile_image\n",
    "    bd_user['geo_enabled'] = user.geo_enabled\n",
    "    bd_user['created_at'] = user.created_at.timestamp()\n",
    "    bd_user['verified'] = user.verified\n",
    "    bd_user['statuses_count'] = user.statuses_count\n",
    "    bd_user['followers_count'] = user.followers_count\n",
    "    bd_user['followees_count'] = user.friends_count\n",
    "    bd_user['favorites_count'] = user.favourites_count\n",
    "    bd_user['listed_count'] = user.listed_count\n",
    "    return bd_user\n",
    "\n",
    "def neo4j_tweet_tostring(tweet):\n",
    "    rt,qt,rp ='','',''\n",
    "    if \"retweeted_status\" in tweet._json and tweet._json[\"retweeted_status\"] is not None:\n",
    "        rt = tweet._json[\"retweeted_status\"]\n",
    "    if \"quoted_status\" in tweet._json and tweet._json[\"quoted_status\"] is not None:\n",
    "        qt = tweet._json[\"quoted_status\"]\n",
    "    if \"in_reply_to_screen_name\" in tweet._json and tweet._json[\"in_reply_to_screen_name\"] is not None:\n",
    "        rp = tweet._json[\"in_reply_to_screen_name\"]\n",
    "        \n",
    "    return '@@@'.join([str(tweet.id),\n",
    "                     str('' if rt else repl(tweet.full_text)),\n",
    "                     str(tweet.created_at.timestamp()),\n",
    "                     str(tweet.favorite_count),\n",
    "                     str(tweet.retweet_count),\n",
    "                     str(rp),\n",
    "                     str(\"\" if not rp else tweet.in_reply_to_status_id),\n",
    "                     str(\"\" if not rp else tweet.in_reply_to_user_id),\n",
    "                     str(qt),\n",
    "                     str(\"\" if not qt and not hasattr(tweet, 'quoted_status') else tweet.quoted_status.id),\n",
    "                     str(\"\" if not qt else tweet.quoted_status.user.id),\n",
    "                     str(\"\" if not qt else repl(tweet.quoted_status.full_text)),\n",
    "                     str(\"\" if not qt else datetime.strptime(str(tweet.quoted_status.created_at), \"%Y-%m-%d %H:%M:%S\").timestamp()),\n",
    "                     str(\"\" if not qt else tweet.quoted_status.favorite_count),\n",
    "                     str(\"\" if not qt else tweet.quoted_status.retweet_count),\n",
    "                     str(rt),\n",
    "                     str(\"\" if not rt else tweet.retweeted_status.id),\n",
    "                     str(\"\" if not rt else tweet.retweeted_status.user.id),\n",
    "                     str(\"\" if not rt else repl(tweet.retweeted_status.full_text)),\n",
    "                     str(\"\" if not rt else tweet.retweeted_status.created_at.timestamp()),\n",
    "                     str(\"\" if not rt else tweet.retweeted_status.favorite_count),\n",
    "                     str(\"\" if not rt else tweet.retweeted_status.retweet_count)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jSaveUser():\n",
    "    def __init__(self, auth_neo4j, auth_tweepy, next_node, n):\n",
    "        self.api = Neo4jSaveUser.auth_tweepy(auth_tweepy)\n",
    "        self.graph = Neo4jSaveUser.auth_neo4j(auth_neo4j)\n",
    "        self.node_selector = NodeMatcher(self.graph)\n",
    "        self.n = n\n",
    "        self.next_node = None\n",
    "        self.previous_node = None\n",
    "        \n",
    "        if self.graph.schema.get_uniqueness_constraints(\"Tweet\") != [\"id\"]:\n",
    "            self.graph.schema.create_uniqueness_constraint(\"Tweet\", \"id\")\n",
    "        if self.graph.schema.get_uniqueness_constraints(\"Multimedia\") != [\"url\"]:\n",
    "            self.graph.schema.create_uniqueness_constraint(\"Multimedia\", \"url\")\n",
    "        if self.graph.schema.get_uniqueness_constraints(\"User\") != [\"id\"]:\n",
    "            self.graph.schema.create_uniqueness_constraint(\"User\", \"id\")\n",
    "        if self.graph.schema.get_indexes(\"User\"):\n",
    "            print('Ya existe el usuario')\n",
    "    \n",
    "    @staticmethod       \n",
    "    def auth_tweepy(auth):\n",
    "        oauth = tweepy.OAuthHandler(auth['consumer_key'], auth['consumer_secret'])\n",
    "        oauth.set_access_token(auth['access_token'], auth['access_secret'])\n",
    "        return tweepy.API(oauth)\n",
    "    \n",
    "    @staticmethod\n",
    "    def auth_neo4j(auth):\n",
    "        return Graph(auth['host'], password=auth['password'])\n",
    "    \n",
    "    @staticmethod\n",
    "    def tweet_urls(tweet):\n",
    "        urls = []\n",
    "        if 'urls' in tweet.entities:\n",
    "            for url in tweet.entities['urls']:\n",
    "                if 'expanded_url' in url and url['expanded_url'] is not None:\n",
    "                    if urlparse(url['expanded_url']).netloc != \"twitter.com\":\n",
    "                        tmp = urlparse(url['expanded_url'])\n",
    "                        urls.append((tmp.netloc+tmp.path, tmp.netloc, tmp.path))\n",
    "        return urls\n",
    "    \n",
    "    @staticmethod\n",
    "    def tweet_quoted(tweet):\n",
    "        if hasattr(tweet, 'quoted_status'):\n",
    "            return [tweet._json['quoted_status']['user']['id']]\n",
    "        return []\n",
    "            \n",
    "    @staticmethod\n",
    "    def tweet_rt_users(tweet):\n",
    "        if hasattr(tweet, 'retweeted_status'):\n",
    "            return [tweet._json['retweeted_status']['user']['id']]\n",
    "        return []\n",
    "    \n",
    "    def user_tweets(self, user):\n",
    "        perfil = self.api.user_timeline(include_rts=True, count=200, trim_user=False, exclude_replies=False, user_id=user['id'], tweet_mode='extended')\n",
    "        tweets, urls, quoted, retweeted = [],[],[],[]\n",
    "        \n",
    "        for t in perfil:\n",
    "            tweets.append(neo4j_tweet_tostring(t))\n",
    "            urls += Neo4jSaveUser.tweet_urls(t)\n",
    "            quoted += Neo4jSaveUser.tweet_quoted(t)\n",
    "            retweeted += Neo4jSaveUser.tweet_rt_users(t)\n",
    "            \n",
    "        tweet = neo4j_tweet(tweets)\n",
    "        self.graph.create(tweet)\n",
    "        self.graph.create(Relationship(user, \"tweeted\", tweet))\n",
    "        \n",
    "        for u in urls:\n",
    "            url = neo4j_multimedia(u)\n",
    "            self.graph.merge(url, \"Multimedia\", 'url')\n",
    "            self.graph.create(Relationship(user, \"shared\", url))\n",
    "        \n",
    "        for node in set(retweeted).union(quoted):\n",
    "            next_node = list(self.node_selector.match(\"User\", id=node))\n",
    "\n",
    "            if len(next_node) == 0:\n",
    "                virtual_user = neo4j_vuser(node)\n",
    "            elif len(next_node) == 1:\n",
    "                virtual_user = next_node[0]\n",
    "            else:\n",
    "                raise Exception(\"Error: varios nodos con el mismo id\")\n",
    "\n",
    "            if node in retweeted:\n",
    "                rel = Relationship(user, \"retweeted\", virtual_user)\n",
    "                self.graph.merge(rel, \"User\", \"id\")\n",
    "            if node in quoted:\n",
    "                rel = Relationship(user, \"quoted\", virtual_user)\n",
    "                self.graph.merge(rel, \"User\", \"id\")\n",
    "        \n",
    "    def push_node(self, node):\n",
    "        user = neo4j_save_info(node, self.api.get_user(node['id']), self.n)\n",
    "        self.n += 1\n",
    "        self.graph.push(user)\n",
    "        self.user_tweets(user)\n",
    "        return user\n",
    "    \n",
    "    def get_adj(self):\n",
    "        ret = []\n",
    "        for rel in self.graph.match(nodes=(self.next_node,), r_type=\"retweeted\"):\n",
    "            print(rel)\n",
    "            ret.append(rel.end_node)\n",
    "        return ret\n",
    "    \n",
    "    def run(self):\n",
    "        for author in authors:\n",
    "            try:\n",
    "                user = neo4j_user(self.api.get_user(author), self.n)\n",
    "                self.graph.merge(user, \"User\", \"id\")\n",
    "                self.user_tweets(user)\n",
    "            except tweepy.TweepError as exception:\n",
    "                print(exception)\n",
    "                continue\n",
    "            self.n += 1\n",
    "            self.next_node = user\n",
    "            if self.next_node:\n",
    "                if self.next_node[\"virtual\"] == \"T\":\n",
    "                    try:\n",
    "                        self.next_node = self.push_node(self.next_node)\n",
    "                        print(\"(Metido {0}\".format(self.next_node[\"screen_name\"]), end=\") \")\n",
    "                    except tweepy.TweepError as exception:\n",
    "                        print(exception)\n",
    "                    except IOError as exception:\n",
    "                        print(exception)\n",
    "                        self.next_node = self.previous_node\n",
    "                        continue\n",
    "                else:\n",
    "                    print(\"(Pasado con {0}\".format(self.next_node[\"screen_name\"]), end=\") \")\n",
    "            else:\n",
    "                print('----------- no se ----------------')\n",
    "                \n",
    "            adj = self.get_adj()\n",
    "            self.previous_node = self.next_node\n",
    "            print(\"Usuario {0} pasado\".format(author))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Neo4jSaveUser(auth_neo4j, auth_tweepy, None, 1)\n",
    "crawler.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardado del grafo\n",
    "Volcado de la base de datos recogida en Neo4j Desktop a **usuarios.graphml** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafonx = nx.DiGraph()\n",
    "grafo = Graph(auth_neo4j['host'], password=auth_neo4j['password'])\n",
    "\n",
    "for nodo in grafo.run(cypher =\"\"\"MATCH (a:User) WHERE a.virtual=\"F\" RETURN a as val\"\"\").data():\n",
    "    vn = dict((nodo['val']))\n",
    "    grafonx.add_node(vn['id'], **vn)\n",
    "    grafonx.add_edge(vn['id'], vn['id'])\n",
    "for nodo in grafo.run(cypher =\"\"\"MATCH (a:User)-[:retweeted]->(b:User) WHERE a.virtual=\"F\" AND b.virtual=\"F\" RETURN a.id as a, b.id as b\"\"\").data():\n",
    "    grafonx.add_edge(nodo['a'], nodo['b'])\n",
    "    \n",
    "nx.write_graphml(grafonx, \"datos_final/usuarios.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nx.draw(grafonx, pos=nx.spring_layout(grafonx))\n",
    "g = nx.read_graphml(\"datos_final/usuarios.graphml\")\n",
    "len(set(list(g.nodes))) ## quitando elementos repetidos!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de ejecutar la recogida de usuarios anterior, finalmente nos quedamos con **3516** usuarios disponibles para analizar, **84** no se han encontrado o han sido suspendidos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardado de los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grafo = Graph(auth_neo4j['host'], password=auth_neo4j['password'])\n",
    "df = grafo.run(cypher=\"\"\"MATCH (u:User) WHERE u.virtual=\"F\" RETURN COUNT(u) AS number\"\"\").data()\n",
    "df[0]['number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos los tweets de la base de datos en **tweets.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla = grafo.run(cypher=\"\"\"MATCH (u:User)-[:tweeted]->(t:Tweet) RETURN u.id as id, u.screen_name as screen_name, t.content as content\"\"\").data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datos_final/tweets.csv', 'w') as file:\n",
    "    csvw = csv.writer(file)\n",
    "    csvw.writerow(['user_id', 'screen_name',\n",
    "                   'tweet_id', 'tweet_text', 'tweet_creation_at', 'tweet_fav_count', 'tweet_rt_count', \n",
    "                   'is_reply', 'reply_id_status', 'reply_id_user',\n",
    "                   'is_quote', 'quote_id_user', 'quote_id_status', 'quote_text', 'quote_creation_at', 'quote_fav_count', 'quote_rt_count',\n",
    "                   'is_rt', 'rt_id_user', 'rt_id_status', 'rt_text', 'rt_creation_at', 'rt_fav_count', 'rt_rt_count'])\n",
    "\n",
    "    for row in tabla:\n",
    "        for tweet in row[\"content\"]:\n",
    "            new_tweet = []\n",
    "            new_tweet = tweet.split(\"@@@\")\n",
    "            if new_tweet[5]:\n",
    "                new_tweet[5] = True\n",
    "            else:\n",
    "                new_tweet[5] = False\n",
    "            if new_tweet[8]:\n",
    "                new_tweet[8] = True\n",
    "            else:\n",
    "                new_tweet[8] = False\n",
    "            if new_tweet[15]:\n",
    "                new_tweet[15] = True\n",
    "            else:\n",
    "                new_tweet[15] = False\n",
    "            \n",
    "    \n",
    "            csvw.writerow([row[\"id\"]] + [row[\"screen_name\"]] + new_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gloriadelvalle/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (4,21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df_tweets = pd.read_csv('datos_final/tweets.csv')\n",
    "#creation_at --> mixed types pero no es problema,  error_bad_lines=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.user_id.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de usuarios de odio\n",
    "Recojo usuarios que utilizan este tipo de léxico. Filtro estos usuarios y quedan como sospechosos de ser *hateful users*. Los guardo en un subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "insult = open(\"lexicon/insults_lexicon.txt\", \"r\")\n",
    "inmigr = open(\"lexicon/immigrant_lexicon.txt\", \"r\")\n",
    "misog = open(\"lexicon/misogyny_lexicon.txt\", \"r\")\n",
    "xeno = open(\"lexicon/xenophobia_lexicon.txt\", \"r\")\n",
    "regexp = \"\"\n",
    "for line in insult.readlines()[1:]:\n",
    "    regexp += \"({0})|\".format(line.rstrip())\n",
    "insult.close()\n",
    "for line in inmigr.readlines()[1:]:\n",
    "    regexp += \"({0})|\".format(line.rstrip())\n",
    "inmigr.close()\n",
    "for line in misog.readlines()[1:]:\n",
    "    regexp += \"({0})|\".format(line.rstrip())\n",
    "misog.close()\n",
    "for line in xeno.readlines()[1:]:\n",
    "    regexp += \"({0})|\".format(line.rstrip())\n",
    "xeno.close()\n",
    "regexp = regexp[:-1]\n",
    "regexp = re.compile(regexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'(abanto)|(abrazafarolas)|(adufe)|(alcornoque)|(alfeñique)|(andurriasmo)|(arrastracueros)|(artabán)|(atarre)|(baboso)|(barrabás)|(barriobajero)|(bebecharcos)|(bellaco)|(belloto)|(berzotas)|(besugo)|(bobalicón)|(bocabuzón)|(bocachancla)|(bocallanta)|(boquimuelle)|(borrico)|(botarate)|(brasas)|(cabestro)|(cabezaalberca)|(cabezabuque)|(cachibache)|(cafre)|(cagalindes)|(cagarruta)|(calambuco)|(calamidad)|(caldúo)|(calientahielos)|(calzamonas)|(cansalmas)|(cantamañanas)|(capullo)|(caracaballo)|(caracartón)|(caraculo)|(caraflema)|(carajaula)|(carajote)|(carapapa)|(carapijo)|(cazurro)|(cebollino)|(cenizo)|(cenutrio)|(ceporro)|(cernícalo)|(charrán)|(chiquilicuatre)|(chirimbaina)|(chupacables)|(chupasangre)|(chupóptero)|(cierrabares)|(cipote)|(comebolsas)|(comechapas)|(comeflores)|(comestacas)|(cretino)|(cuerpoescombro)|(culopollo)|(descerebrado)|(desgarracalzas)|(dondiego)|(donnadie)|(echacantos)|(ejarramantas)|(energúmeno)|(esbaratabailes)|(eEscolimoso)|(escornacabras)|(estulto)|(fanfosquero)|(fantoche)|(fariseo)|(filimincias)|(foligoso)|(fulastre)|(ganapán)|(ganapio)|(gandúl)|(gañán)|(gaznápiro)|(gilipuertas)|(giraesquinas)|(gorrino)|(gorrumino)|(guitarro)|(gurriato)|(habahelá)|(huelegateras)|(huevón)|(lamecharcos)|(lameculos)|(lameplatos)|(lechuguino)|(lerdo)|(letrín)|(lloramigas)|(longanizas)|(lumbreras)|(maganto)|(majadero)|(malasangre)|(malasombra)|(malparido)|(mameluco)|(mamporrero)|(manegueta)|(mangarrán)|(mMangurrián)|(mastuerzo)|(matacandiles)|(meapilas)|(melón)|(mendrugo)|(mentecato)|(mequetrefe)|(merluzo)|(metemuertos)|(metijaco)|(mindundi)|(morlaco)|(morroestufa)|(muerdesartenes)|(orate)|(ovejo)|(pagafantas)|(palurdo)|(pamplinas)|(panarra)|(panoli)|(papafrita)|(papanatas)|(papirote)|(paquete)|(pardillo)|(parguela)|(pasmarote)|(pasmasuegras)|(pataliebre)|(patán)|(pavitonto)|(pazguato)|(pecholata)|(pedorro)|(peinabombillas)|(peinaovejas)|(pelagallos)|(pelagambas)|(pelagatos)|(pelatigres)|(pelazarzas)|(pelele)|(pelma)|(percebe)|(perrocostra)|(perroflauta)|(peterete)|(petimetre)|(picapleitos)|(pichabrava)|(pillavispas)|(piltrafa)|(pinchauvas)|(pintamonas)|(piojoso)|(pitañoso)|(pitofloro)|(plomo)|(pocasluces)|(pollopera)|(quitahipos)|(rastrapajo)|(rebañasandías)|(revientabaules)|(ríeleches)|(robaperas)|(sabandija)|(sacamuelas)|(sanguijuela)|(sinentraero)|(sinsustancia)|(sonajas)|(sonso)|(soplagaitas)|(soplaguindas)|(sosco)|(sagarote)|(tarado)|(tarugo)|(tiralevitas)|(tocapelotas)|(tocho)|(tolai)|(tontaco)|(tontucio)|(tordo)|(tragaldabas)|(tuercebotas)|(tunante)|(zamacuco)|(zambombo)|(zampabollos)|(zamugo)|(zángano)|(zarrapastroso)|(zascandil)|(zopenco)|(zoquete)|(zote)|(zullenco)|(zurcefrenillos)|(asno)|(bastardo)|(Bollera)|(abron)|(abrón)|(cabron)|(cabrón)|(caca)|(chupada)|(chupapollas)|(coño)|(coprofagía)|(gilipichis)|(gilipollas)|(heroína)|(hijadeputa)|(hijaputa)|(hijodeputa)|(hijoputa)|(idiota)|(imbécil)|(infierno)|(jilipollas)|(capullo)|(lameculos)|(maciza)|(macizorra)|(maldito)|(maldita)|(malditos)|(malditas)|(mamada)|(marica)|(maricón)|(mariconazo)|(martillo)|(mierda)|(nazi)|(pedo)|(pervertido)|(pezón)|(pinche)|(prostituta)|(racista)|(ramera)|(sádico)|(soplagaitas)|(soplapollas)|(travesti)|(verga)|(vulva)|(puto)|(putos)|(puta)|(putas)|(zorra)|(zorras)|(africano)|(africana)|(africanos)|(africanas)|(arabe)|(arabes)|(árabe)|(árabes)|(aracuano)|(aracuana)|(aracuanos)|(aracuanas)|(boludo)|(boluda)|(boludos)|(boludas)|(cambujo)|(cambuja)|(cambujos)|(cambujas)|(chale)|(champinon)|(chapin)|(chapina)|(checo)|(checa)|(checos)|(checas)|(Chilango)|(chilanga)|(chilangos)|(chilangas)|(clandestina)|(clandestino)|(clandestinos)|(clandestinas)|(defeño)|(deportará)|(deportarlo)|(deportarla)|(deportarlos)|(deportarlas)|(deporten)|(emigrante)|(españolito)|(españolita)|(españolitos)|(españolitas)|(francés)|(franchute)|(franco)|(gabacho)|(gabacha)|(gabachos)|(gabachas)|(galo)|(gala)|(galos)|(galas)|(guacha)|(guacho)|(guachas)|(guahos)|(guajiro)|(guajira)|(guajiros)|(guajiras)|(guatemalteco)|(guatemalteca)|(guatemaltecos)|(guatemaltecas)|(guiri)|(guiris)|(húngaro)|(húngara)|(húngarás)|(húngaras)|(ilegal)|(ilegales)|(ilegalidad)|(ilegalidad)|(ilegalmente)|(indocumentada)|(indocumentado)|(indocumentados)|(indocumentadas)|(inmigrante)|(inmigrantes)|(migrante)|(migrantes)|(judas)|(judio)|(judios)|(judíos)|(judío)|(lituano)|(lituana)|(lituanos)|(lituanas)|(mestizo)|(metiza)|(mestizos)|(mestizas)|(migrante)|(migratorio)|(mojo)|(moriego)|(musulman)|(mulato)|(mulata)|(multatos)|(mulatas)|(papión)|(pelotudo)|(pelotudos)|(pelotudas)|(pelotuda)|(pocha)|(pocho)|(pochos)|(pochas)|(polaca)|(polaco)|(polacos)|(polacas)|(polonés)|(transmigrante)|(zambaggoa)|(zambo)|(zambos)|(zamba)|(zambas)|(extranjero)|(extranjera)|(extranjeros)|(expatriado)|(expatriada)|(expratriados)|(carcamán)|(refugiado)|(refugiada)|(refugiados)|(refugiadas)|(latinos)|(latinas)|(latino)|(latina)|(centroamericano)|(centroamericana)|(centroamericanos)|(centroamericanas)|(moreno)|(morenos)|(subsahariano)|(subsahariana)|(subsaharianas)|(subsaharianos)|(catracho)|(catracha)|(catrachos)|(catrachas)|(chapetón)|(chapetones)|(charnegos)|(charnego)|(chicano)|(chicanos)|(cholo)|(cholos)|(gachupín)|(gachupines)|(gallego)|(gallegos)|(godo)|(godos)|(gringo)|(gringos)|(gringa)|(gringas)|(kurepa)|(kurepas)|(llanito)|(llanitos)|(maqueto)|(maquetos)|(maketo)|(maketos)|(negro)|(negros)|(negra)|(negras)|(yanqui)|(yanquis)|(yanki)|(yankis)|(yuma)|(yumas)|(argentina)|(boliviana)|(brasileña)|(chilena)|(colombiana)|(ecuatoriana)|(guayanés)|(paraguaya)|(peruana)|(surinamés)|(uruguaya)|(venezolana)|(argentino)|(boliviano)|(brasileño)|(chileno)|(colombiano)|(ecuatoriano)|(guayanesa)|(paraguayo)|(peruano)|(surinamesa)|(uruguayo)|(venezolano)|(chino)|(china)|(argentinas)|(bolivianas)|(brasileñas)|(chilenas)|(colombianas)|(ecuatorianas)|(guayanés)|(paraguayas)|(peruanas)|(surinamés)|(uruguayas)|(venezolanas)|(argentinos)|(bolivianos)|(brasileños)|(chilenos)|(colombianos)|(ecuatorianos)|(guayanesas)|(paraguayos)|(peruanos)|(surinamesas)|(uruguayos)|(venezolanos)|(chinos)|(chinas)|(alcahueta)|(alemanita)|(amante)|(amargada)|(anabolena)|(arpía)|(asquerosa)|(avariuda)|(bagasa)|(barragana)|(barriobajera)|(bataclana)|(bordiona)|(burraca)|(burracona)|(buscona)|(bruja)|(cabaretera)|(cabrona)|(calentorra)|(calientacamas)|(calientahuevos)|(calientapollas)|(cerda)|(cambri)|(candonga)|(cantonera)|(casquivana)|(celestina)|(celosa)|(chai)|(chango)|(chingada)|(chirlata)|(choni)|(choriza)|(chumascona)|(chuminista)|(chupapijas)|(chupetera)|(churrera)|(cisne)|(cleopatra)|(cocota)|(cocu)|(coima)|(colipoterra)|(concha)|(conchita)|(concubina)|(coneja)|(cortesana)|(costillera)|(cualquiera)|(currulaca)|(daifa)|(descarriada)|(deshonesta)|(disoluta)|(diva)|(elementa)|(empaná)|(enrayada)|(envidiosa)|(espatarrada)|(esquinera)|(facilona)|(falsa)|(fea)|(feminazi)|(folladora)|(fresca)|(fuellera)|(fulana)|(furcia)|(golfa)|(guarra)|(gueisa)|(gumia)|(hetaira)|(huevera)|(hurgamandera)|(hurona)|(iza)|(jabata)|(lacroilla)|(ladillera)|(lagarta)|(lagartera)|(lagartona)|(lea)|(leona)|(leonesa)|(libertina)|(ligerita)|(loca)|(lumi)|(lumia)|(lumiasca)|(madam)|(mala)|(malparida)|(malparia)|(mamadera)|(mamona)|(manceba)|(manfla)|(manipuladora)|(mentirosa)|(meretriz)|(mesalina)|(microondas)|(mina)|(moma)|(mona)|(mortadela)|(mozcorra)|(mujerzuela)|(niñata)|(nocturna)|(ordeñadora)|(pájara)|(pajarera)|(pajarona)|(pajillera)|(pantera)|(patín)|(patinadora)|(pelandusca)|(pendeja)|(pendón)|(perendeca)|(perica)|(perra)|(pesetera)|(piculina)|(pilingui)|(pingona)|(polilla)|(pollera)|(puerca)|(pringá)|(pringa)|(pringada)|(prosti)|(prostituta)|(pujicama)|(pupila)|(puta)|(quedona)|(ramera)|(rastrera)|(rebuscona)|(rufa)|(sabanera)|(soldadera)|(sota)|(subidita)|(sumisa)|(suripanta)|(taconera)|(tierrosa)|(tigresa)|(tipa)|(tiparraca)|(tortillera)|(trabuquera)|(trotacalles)|(trotadora)|(trotera)|(trotona)|(trufera)|(turquesa)|(verdulera)|(vieja)|(vulgar)|(zorra)|(zurriaga)|(terruca)|(cochina)|(ladrona)|(ratera)|(traicionera)|(indio)|(machupichus)|(machupichu)|(marrano)|(marrana)|(marranos)|(marranas)|(moromierda)|(moromierdas)|(musulmono)|(musulmonos)|(moro)|(mora)|(moros)|(moras)|(negrata)|(negratas)|(payoponi)|(payoponis)|(mono)|(monos)|(rumano)|(rumanos)|(rumanas)|(rumana)|(salvaje)|(salvajes)|(sudaca)|(sudacas)|(sudaka)|(sudakas)|(frijolero)|(frijoleros)|(frijolera)|(frijoleras)|(tiraflechas)|(simio)|(simios)|(panchito)|(panchita)|(panchitos)|(panchitas)|(manteros)|(mantero)',\n",
       "re.UNICODE)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"datos_final/tweets.csv\", \"r\")\n",
    "re.match(regexp, \"\")\n",
    "csv_writer = csv.DictReader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_users = {}\n",
    "for line in csv_writer:\n",
    "    text = regexp.search(line[\"tweet_text\"])\n",
    "    retweet = regexp.search(line[\"rt_text\"])\n",
    "    quote = regexp.search(line[\"quote_text\"])\n",
    "    if text is not None or retweet is not None or quote is not None:\n",
    "        set_users[line[\"user_id\"]] = True\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#with open('datos_final/usuarios_mal_lexico.pickle', 'wb') as handle:\n",
    "#    pickle.dump(set_users, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('datos_final/usuarios_mal_lexico.pickle', 'rb') as handle:\n",
    "    set_users = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_users2 = {}\n",
    "for k,v in set_users.items():\n",
    "    if k in set(list(grafo_nx.nodes)):\n",
    "        set_users2[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafo_nx = nx.read_graphml(\"datos_final/usuarios.graphml\")\n",
    "grafo_nx = grafo_nx.reverse(copy=False)\n",
    "nx.set_node_attributes(grafo_nx, name=\"mal_lexico\", values=set_users)\n",
    "nx.write_graphml(grafo_nx, \"datos_final/usuarios_mal_lexico.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modificacion tras suspension de 36 usuarios\n",
    "grafo_nx = nx.readwrite.graphml.read_graphml(\"datos_final/usuarios.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv1 = pd.read_csv('datos_final/usuarios_a_etiquetar.csv')\n",
    "df_all_users =  pd.read_csv('exec_users/df_all_users.csv')\n",
    "usuarios_baja = set(csv1.user_id.tolist()) - set(df_all_users.user_id.tolist())\n",
    "for user in list(usuarios_baja):\n",
    "    grafo_nx.remove_node(str(user))\n",
    "nx.write_graphml(grafo_nx, \"datos_final/usuarios.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "743"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = nx.read_graphml(\"datos_final/usuarios_mal_lexico.graphml\")\n",
    "len(list(g.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = pd.read_csv('datos_final/usuarios_a_etiquetar.csv') ## etiqueta con haterbert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de las características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_usuarios = {}\n",
    "with open(\"datos_final/usuarios_etiquetados.csv\", \"r\") as f:\n",
    "    csv_writer=csv.DictReader(f)\n",
    "    for l in csv_writer:\n",
    "        if l['hate'] == '1':\n",
    "            set_usuarios[l['user_id']] = 1\n",
    "        elif l['hate'] == '0':\n",
    "            set_usuarios[l['user_id']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodos = grafo_nx.nodes(data='hate')\n",
    "\n",
    "haters = {}\n",
    "normales = {}\n",
    "\n",
    "for n in nodos:\n",
    "    if n[1] == 1:  # hater\n",
    "        for i in grafo_nx.neighbors(n[0]):\n",
    "            haters[i] = True\n",
    "    if n[1] == 0: # normal\n",
    "        for i in grafo_nx.neighbors(n[0]):\n",
    "            normales[n] = True\n",
    "\n",
    "nx.set_node_attributes(grafo_nx, name=\"hater\", values=False)\n",
    "nx.set_node_attributes(grafo_nx, name=\"hater\", values=haters)\n",
    "nx.set_node_attributes(grafo_nx, name=\"normal\", values=False)\n",
    "nx.set_node_attributes(grafo_nx, name=\"normal\", values=normales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "743"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(grafo_nx.nodes)) ## elijo k = 3516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1247806839305031680': 16.5, '1174721480850104320': 17.0, '1331574487788228608': 6.0, '1387868813023186955': 4.0, '1923040616': 3.5, '3184834521': 4.0, '454351037': 1.0, '1353391775373811712': 1.0, '2755884282': 2.0, '468313376': 1.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grafo_nx = nx.read_graphml(\"datos_final/usuarios.graphml\")\n",
    "#nx.set_node_attributes(grafo_nx, 0.1, 'percolation')\n",
    "\n",
    "betweenness = nx.betweenness_centrality(grafo_nx, normalized =False, endpoints = False) #, normalized=False\n",
    "#eigenvector = nx.eigenvector_centrality(grafo_nx)\n",
    "in_degree = nx.in_degree_centrality(grafo_nx)\n",
    "out_degree = nx.out_degree_centrality(grafo_nx)\n",
    "#a_clustering = nx.average_clustering(grafo_nx)\n",
    "clustering = nx.clustering(grafo_nx) # ver si funciona\n",
    "degree = nx.degree_centrality(grafo_nx) #\n",
    "closeness = nx.closeness_centrality(grafo_nx) #\n",
    "#percolation = nx.percolation_centrality(G=grafo_nx, attribute='percolation') #\n",
    "#dispersion = nx.dispersion(grafo_nx) # no\n",
    "#voterank = nx.voterank(grafo_nx) #\n",
    "res = {i:v for i,v in betweenness.items() if betweenness[i] > 0.0}\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(eigenvector.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#nx.betweenness_centrality(grafo_nx) ### esto es porque todos estan conectados \n",
    "eigenvector.update({'626174845': a})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(grafo_nx, name=\"betweenness\", values=betweenness)\n",
    "nx.set_node_attributes(grafo_nx, name=\"eigenvector\", values=eigenvector)\n",
    "nx.set_node_attributes(grafo_nx, name=\"in_degree\", values=in_degree)\n",
    "nx.set_node_attributes(grafo_nx, name=\"out_degree\", values=out_degree)\n",
    "nx.set_node_attributes(grafo_nx, name=\"clustering\", values=clustering)\n",
    "nx.set_node_attributes(grafo_nx, name=\"degree\", values=degree)\n",
    "nx.set_node_attributes(grafo_nx, name=\"closeness\", values=closeness)\n",
    "#nx.set_node_attributes(grafo_nx, name=\"percolation\", values=percolation)\n",
    "#nx.set_node_attributes(grafo_nx, name=\"dispersion\", values=dispersion)\n",
    "#nx.set_node_attributes(grafo_nx, name=\"voterank\", values=voterank)\n",
    "\n",
    "nx.write_graphml(grafo_nx, \"datos_final/usuarios_hate_centralidad.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "grafo_nx = nx.read_graphml(\"datos_final/usuarios_hate_centralidad.graphml\")\n",
    "\n",
    "hate = nx.get_node_attributes(grafo_nx, \"hate\")\n",
    "\n",
    "vec_hate = nx.get_node_attributes(grafo_nx, \"hater\")\n",
    "vec_normal = nx.get_node_attributes(grafo_nx, \"normal\")\n",
    "\n",
    "uname = nx.get_node_attributes(grafo_nx, \"uname\")\n",
    "usuarios_nouname = set(set_usuarios.keys()) - set(uname.keys())\n",
    "aux = dict.fromkeys(usuarios_nouname, math.nan)\n",
    "uname = {**uname, **aux}\n",
    "\n",
    "screen_name = nx.get_node_attributes(grafo_nx, \"screen_name\")\n",
    "\n",
    "description = nx.get_node_attributes(grafo_nx, \"description\")\n",
    "usuarios_nodescr = set(set_usuarios.keys()) - set(description.keys())\n",
    "aux = dict.fromkeys(usuarios_nodescr, math.nan)\n",
    "description = {**description, **aux}\n",
    "#lang = nx.get_node_attributes(grafo_nx, \"lang\")\n",
    "\n",
    "location = nx.get_node_attributes(grafo_nx, \"location\")\n",
    "usuarios_noloc = set(set_usuarios.keys()) - set(location.keys())\n",
    "aux = dict.fromkeys(usuarios_noloc, math.nan)\n",
    "location = {**location, **aux}\n",
    "\n",
    "created_at = nx.get_node_attributes(grafo_nx, \"created_at\")\n",
    "verified = nx.get_node_attributes(grafo_nx, \"verified\")\n",
    "statuses_count = nx.get_node_attributes(grafo_nx, \"statuses_count\")\n",
    "followers_count = nx.get_node_attributes(grafo_nx, \"followers_count\")\n",
    "followees_count = nx.get_node_attributes(grafo_nx, \"followees_count\")\n",
    "favorites_count = nx.get_node_attributes(grafo_nx, \"favorites_count\")\n",
    "listed_count = nx.get_node_attributes(grafo_nx, \"listed_count\")\n",
    "#time_zone = nx.get_node_attributes(grafo_nx, \"time_zone\")\n",
    "geo_enabled = nx.get_node_attributes(grafo_nx, \"geo_enabled\")\n",
    "\n",
    "prof_img = nx.get_node_attributes(grafo_nx, \"profile_image_url\")\n",
    "usuarios_noimg = set(set_usuarios.keys()) - set(prof_img.keys())\n",
    "aux = dict.fromkeys(usuarios_noimg, math.nan)\n",
    "prof_img = {**prof_img, **aux}\n",
    "\n",
    "default_prof = nx.get_node_attributes(grafo_nx, \"default_profile\")\n",
    "default_prof_im = nx.get_node_attributes(grafo_nx, \"default_profile_image\")\n",
    "\n",
    "betweenness = nx.get_node_attributes(grafo_nx, \"betweenness\")\n",
    "eigenvector = nx.get_node_attributes(grafo_nx, \"eigenvector\")\n",
    "in_degree = nx.get_node_attributes(grafo_nx, \"in_degree\")\n",
    "out_degree = nx.get_node_attributes(grafo_nx, \"out_degree\")\n",
    "clustering = nx.get_node_attributes(grafo_nx, \"clustering\")\n",
    "degree = nx.get_node_attributes(grafo_nx, \"degree\")\n",
    "closeness = nx.get_node_attributes(grafo_nx, \"closeness\")\n",
    "#percolation = nx.get_node_attributes(grafo_nx, \"percolation\")\n",
    "#dispersion = nx.get_node_attributes(grafo_nx, \"dispersion\")\n",
    "#voterank = nx.get_node_attributes(grafo_nx, \"voterank\")\n",
    "\n",
    "tweet = TweetTokenizer()\n",
    "for k,v in description.items():\n",
    "    #new_val = re.sub(\"s+\",\" \", str(v))\n",
    "    new_val = re.sub(\"[^-9A-Za-z ]\", \"\" , str(v))\n",
    "    new_val = tweet.tokenize(str(new_val))\n",
    "    #new_val = \" \".join([i for i in new_val if i not in string.punctuation])\n",
    "    new_val = \" \".join([i for i in new_val])\n",
    "    words = nltk.tokenize.word_tokenize(new_val)\n",
    "    new_val = [i for i in words if i not in stopwords]\n",
    "    description[k] = ' '.join(str(x) for x in new_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "usuarios = []\n",
    "for user_id in hate.keys():\n",
    "    #hater = \"otro\"\n",
    "   # if hate[user_id] == 1:\n",
    "   #     hater = \"hater\"\n",
    "   # elif hate[user_id] == 0:\n",
    "    #    hater = \"normal\"\n",
    "    #description[user_id] = str(description[user_id])\n",
    "    usuarios.append((user_id,\n",
    "                     #hater,\n",
    "                     #vec_hate[user_id],\n",
    "                     #vec_normal[user_id],\n",
    "                     #uname[user_id], \n",
    "                     #screen_name[user_id], \n",
    "                     #description[user_id], \n",
    "                     #location[user_id], \n",
    "                     #created_at[user_id], \n",
    "                     #verified[user_id],\n",
    "                     #statuses_count[user_id],\n",
    "                     #followers_count[user_id],\n",
    "                     #followees_count[user_id],\n",
    "                     #favorites_count[user_id],\n",
    "                     #listed_count[user_id],\n",
    "                     #geo_enabled[user_id],\n",
    "                     #prof_img[user_id], \n",
    "                     #default_prof[user_id],\n",
    "                     #default_prof_im[user_id],\n",
    "                     betweenness[user_id],\n",
    "                     eigenvector[user_id],\n",
    "                     in_degree[user_id],\n",
    "                     out_degree[user_id],\n",
    "                     clustering[user_id],\n",
    "                     degree[user_id],\n",
    "                     closeness[user_id]\n",
    "                    ))\n",
    "    #, percolation[user_id], dispersion[user_id], voterank[user_id], time_zone[user_id] lang[user_id],\n",
    "\n",
    "cols = [\"user_id\", \n",
    "        #\"hate\", \n",
    "        #\"hate_vecino\", \n",
    "        #\"normal_vecino\", \n",
    "        #\"nombre_perfil\", \n",
    "        #\"screen_name\", \n",
    "        #\"descripcion\", \n",
    "        #\"localizacion\", \n",
    "        #\"fecha_creacion\", \n",
    "        #\"verificado\",\n",
    "        #\"statuses_count\", \n",
    "        #\"followers_count\", \n",
    "        #\"followees_count\", \n",
    "        #\"favorites_count\", \n",
    "        #\"listed_count\", \n",
    "        #\"geo_enabled\",\n",
    "        #\"profile_image_url\", \n",
    "        #\"profile_changed\", \n",
    "        #\"img_prof_changed\",\n",
    "        \"betweenness\", \n",
    "        \"eigenvector\", \n",
    "        \"in_degree\", \n",
    "        \"out_degree\",\n",
    "        \"clustering\", \n",
    "        \"degree\", \n",
    "        \"closeness\"]\n",
    "#, \"percolation\", \"dispersion\", \"voterank\", \"time_zone\" \"idioma\",\n",
    "\n",
    "df = pd.DataFrame.from_records(usuarios, columns=cols)\n",
    "df.to_csv(\"datos_final/usuarios_caracteristicas_centralidad_arregladas.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de las características textuales 1.0 (ahora está implementado en acc_analyzer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1 = pd.read_csv(\"datos_final/usuarios_caracteristicas.csv\")\n",
    "#df2 = pd.read_csv(\"exec_users/df_all_users.csv\")\n",
    "#df1 = pd.read_csv(\"datos_final_743/usuarios_caracteristicas_centralidad_arregladas.csv\")\n",
    "#df2 = pd.read_csv(\"datos_final_743/usuarios_caracteristicas_finales.csv\")\n",
    "df1 = df1[[\"user_id\",\n",
    "           #\"hate\",\n",
    "           \"hate_vecino\",\n",
    "           #\"normal_vecino\",\n",
    "           \"nombre_perfil\",\"screen_name\",\"descripcion\",\"localizacion\",\"fecha_creacion\",\"verificado\",\n",
    "           \"statuses_count\",\"followers_count\",\"followees_count\",\"favorites_count\",\"listed_count\",\"profile_changed\",\"img_prof_changed\",\n",
    "           \"betweenness\",\"eigenvector\",\"in_degree\",\"out_degree\",\"clustering\",\"degree\",\"closeness\"]]\n",
    "#df3 = df2\n",
    "df2 = df2[[\"user_id\",\n",
    "           #\"hate_x\",\n",
    "           #\"hate_vecino\",\n",
    "           #\"normal_vecino\",\"nombre_perfil\",\"screen_name\",\"descripcion\",\"localizacion\",\"fecha_creacion\",\"verificado\",\n",
    "           #\"statuses_count\",\"followers_count\",\"followees_count\",\"favorites_count\",\"listed_count\",\"profile_changed\",\"img_prof_changed\",\n",
    "            \"user_name\",\"user_geo_enabled\",\"profile_image_url\",\"categories_profile_image_url\",\"status_count\",\"status_retrieving\",\n",
    "            \"status_start_date\",\"status_end_date\",\"status_days\",\"status_note\",\"status_average_tweets_per_day\",\n",
    "            \"activity_hourly_00:00\",\"activity_hourly_01:00\",\"activity_hourly_02:00\",\"activity_hourly_03:00\",\"activity_hourly_04:00\",\n",
    "            \"activity_hourly_05:00\",\"activity_hourly_06:00\",\"activity_hourly_07:00\",\"activity_hourly_08:00\",\"activity_hourly_09:00\",\"activity_hourly_10:00\",\n",
    "            \"activity_hourly_11:00\",\"activity_hourly_12:00\",\"activity_hourly_13:00\",\"activity_hourly_14:00\",\"activity_hourly_15:00\",\"activity_hourly_16:00\",\n",
    "            \"activity_hourly_17:00\",\"activity_hourly_18:00\",\"activity_hourly_19:00\",\"activity_hourly_20:00\",\"activity_hourly_21:00\",\"activity_hourly_22:00\",\n",
    "            \"activity_hourly_23:00\",\"activity_weekly_0\",\"activity_weekly_1\",\"activity_weekly_2\",\"activity_weekly_3\",\"activity_weekly_4\",\"activity_weekly_5\",\n",
    "            \"activity_weekly_6\",\"top_languages\",\"top_sources\",\"geo_enabled_tweet_count\",\"top_places\",\"num_hashtags\",\"top_hashtags\",\"rt_count\",\"top_retweeted_users\",\n",
    "            \"num_mentions\",\"top_mentioned_users\",\"num_urls\",\"top_referenced_domains\",\"negativos\",\"positivos\",\"neutros\",\"hate\",\"no_hate\",\"negativos_score\",\n",
    "            \"positivos_score\",\"neutros_score\",\"hate_score\",\"no_hate_score\",\"is_hater\",\"baddies\",\"n_baddies\",\"n_baddies_tweet\",\"len_status\",\"times_user_rt\",\n",
    "            \"times_user_quotes\",\"num_rts_to_tweets\",\"num_favs_to_tweets\",\"top_categories\",\"misspelling_counter\",\"leet_counter\"]]\n",
    "df = pd.merge(df2, df1, on=\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"all datasets/labeled_reduced_caracteristicas_finales.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final.loc[df_final.label == 1]\n",
    "df_final.to_csv(\"all datasets/labeled_reduced_1487.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all datasets/labeled_reduced_caracteristicas_finales.csv')\n",
    "df_texto = pd.read_csv('all datasets/tweet_hr_label_6K.csv')\n",
    "df_texto = df_texto.rename(columns={\"author_id\": \"user_id\"})\n",
    "df_texto = df_texto[['user_id', 'text', 'label']]\n",
    "df_final = pd.merge(df_texto, df, on=\"user_id\")\n",
    "df_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
